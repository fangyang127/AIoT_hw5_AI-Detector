import io
import math
import re
from typing import Dict, List, Optional, Tuple

import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline


# æ”¯æ´å¤šå€‹æ¨¡å‹ä¸¦å¿«å–ï¼Œæ–¹ä¾¿ ensemble
@st.cache_resource(show_spinner=False)
def load_detector(model_name: str):
    return pipeline(
        "text-classification",
        model=model_name,
        device=-1,  # CPU
    )


@st.cache_resource(show_spinner=False)
def load_ppl_model():
    """è¼‰å…¥ GPT2 ç”¨æ–¼å›°æƒ‘åº¦ä¼°è¨ˆï¼ˆå°æ¨¡å‹ï¼ŒCPU å¯æ¥å—ï¼‰ã€‚"""
    tok = AutoTokenizer.from_pretrained("gpt2")
    mdl = AutoModelForCausalLM.from_pretrained("gpt2")
    return tok, mdl


def read_uploaded_file(file) -> str:
    """å°‡ä¸Šå‚³æª”æ¡ˆå…§å®¹è§£ç¢¼æˆæ–‡å­—ï¼Œå¿½ç•¥ç„¡æ³•è§£ç¢¼çš„å­—å…ƒã€‚"""
    try:
        return file.read().decode("utf-8", errors="ignore")
    except Exception:
        return ""


def _heuristic_ai_boost(text: str) -> float:
    """
    è‹¥æ–‡æœ¬åŒ…å«å¸¸è¦‹ LLM è‡ªæˆ‘æè¿°èªå¥ï¼Œå° AI æ©Ÿç‡åšå¾®å¹…åŠ æ¬Šã€‚
    é€™äº›ç‰‡èªåœ¨çœŸå¯¦äººé¡æ–‡æœ¬ä¸­å°‘è¦‹ï¼Œå¯æå‡åµæ¸¬æº–ç¢ºåº¦ã€‚
    """
    patterns = [
        r"\bas an ai language model\b",
        r"\bi do not have access to real[- ]time data\b",
        r"\bi don't have browsing capabilities\b",
        r"\bhere (is|are) (a|some) (concise|brief)\b",
        r"\bprovide (bullet points|a summary)\b",
        r"\bi cannot provide personal experiences\b",
        r"\bi am an artificial intelligence\b",
        r"\bi'm an ai\b",
        r"\bglad to help\b",
        r"\bassistant\b",
        r"\bas an assistant\b",
        r"\bi cannot fulfill that request\b",
        r"\bi don't have feelings\b",
    ]
    text_lower = text.lower()
    return 0.6 if any(re.search(p, text_lower) for p in patterns) else 0.0


def _gpt2_perplexity(text: str) -> Optional[float]:
    """è¨ˆç®— GPT2 å›°æƒ‘åº¦ï¼Œæ–‡æœ¬éçŸ­æ™‚è¿”å› Noneã€‚"""
    if len(text.split()) < 8:
        return None
    tok, mdl = load_ppl_model()
    enc = tok(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        out = mdl(**enc, labels=enc["input_ids"])
        loss = out.loss
    return math.exp(loss.item())


def predict(text: str) -> Optional[Tuple[float, float, float, Dict[str, float]]]:
    """
    å›å‚³ (ai_prob, human_prob, max_confidence, breakdown)
    - ai_prob: Fake æ¨™ç±¤åˆ†æ•¸
    - human_prob: Real æ¨™ç±¤åˆ†æ•¸
    - max_confidence: æœ€é«˜åˆ†æ•¸ï¼Œç”¨æ–¼ä½ä¿¡å¿ƒæç¤º
    - breakdown: ç´€éŒ„å„æ¨¡å‹è¼¸å‡ºï¼Œä¾¿æ–¼é™¤éŒ¯
    """
    text = (text or "").strip()
    if not text:
        return None

    # è¼•é‡æ¨¡å‹æŠ•ç¥¨ï¼šOpenAI detector + ChatGPT detector
    model_names = [
        "roberta-base-openai-detector",  # Fake / Real
        "Hello-SimpleAI/chatgpt-detector-roberta",  # ChatGPT / Human
    ]

    ai_scores: List[Tuple[float, float]] = []  # (ai, weight)
    human_scores: List[Tuple[float, float]] = []
    breakdown: Dict[str, float] = {}

    for name in model_names:
        clf = load_detector(name)
        outputs = clf(
            text,
            truncation=True,
            max_length=512,
            return_all_scores=True,
        )[0]
        score_map = {o["label"].lower(): float(o["score"]) for o in outputs}

        if "fake" in score_map and "real" in score_map:
            ai_scores.append(score_map["fake"])
            human_scores.append(score_map["real"])
            breakdown[f"{name}_ai"] = score_map["fake"]
            breakdown[f"{name}_human"] = score_map["real"]
        elif "chatgpt" in score_map and "human" in score_map:
            ai_scores.append(score_map["chatgpt"])
            human_scores.append(score_map["human"])
            breakdown[f"{name}_ai"] = score_map["chatgpt"]
            breakdown[f"{name}_human"] = score_map["human"]

    if not ai_scores or not human_scores:
        return None

    # ä¾å„æ¨¡å‹ç½®ä¿¡åº¦ (|ai-human|) åŠ æ¬Šå¹³å‡ï¼Œåå‘é«˜ç½®ä¿¡æ¨¡å‹
    ai_prob = 0.0
    human_prob = 0.0
    weight_sum = 0.0
    for ai, human in zip(ai_scores, human_scores):
        ai_val = ai if isinstance(ai, float) else ai
        human_val = human if isinstance(human, float) else human
        weight = max(abs(ai_val - human_val), 0.1)
        ai_prob += ai_val * weight
        human_prob += human_val * weight
        weight_sum += weight
    ai_prob = ai_prob / weight_sum
    human_prob = human_prob / weight_sum

    # é‡å°æ˜é¡¯ LLM ç‰‡èªåšåŠ æ¬Šèˆ‡ä¸‹é™æå‡ï¼ˆå¼·åˆ¶åå‘ AIï¼‰
    heuristic = _heuristic_ai_boost(text)
    if heuristic > 0:
        ai_prob = 0.95
        human_prob = 0.05
    else:
        # ä½¿ç”¨ GPT2 å›°æƒ‘åº¦ä½œç‚ºè¼”åŠ©ï¼šä½å›°æƒ‘åº¦ä»£è¡¨è¼ƒåƒæ¨¡å‹ç”Ÿæˆ
        ppl = _gpt2_perplexity(text)
        if ppl is not None:
            if ppl < 15:
                ai_prob += 0.25
            elif ppl < 30:
                ai_prob += 0.15

    # æ­£è¦åŒ–è®“ AI% + Human% = 1
    total = ai_prob + human_prob
    if total > 0:
        ai_prob = ai_prob / total
        human_prob = human_prob / total
    else:
        human_prob = 1.0 - ai_prob

    max_confidence = max(ai_prob, human_prob)
    return ai_prob, human_prob, max_confidence, breakdown


st.set_page_config(
    page_title="AI / Human æ–‡ç« åµæ¸¬å™¨",
    page_icon="ğŸ§­",
    layout="centered",
)

st.title("ğŸ§­ AI / Human æ–‡ç« åµæ¸¬å™¨")
st.write(
    "è¼¸å…¥ä¸€æ®µæ–‡æœ¬æˆ–ä¸Šå‚³æ–‡å­—æª”ï¼Œç«‹å³ä¼°è¨ˆè©²æ®µæ–‡å­—ç‚º **AI ç”Ÿæˆ** æˆ– **äººé¡æ’°å¯«** çš„æ©Ÿç‡ã€‚"
)

# é è¨­æ¨£ä¾‹
sample_texts = {
    "AI ç¯„ä¾‹": (
        "Certainly! Here is a concise, well-structured overview of the requested topic. "
        "As an AI language model, I will provide bullet points, a short summary, and a "
        "polite closing statement to ensure clarity and coherence."
    ),
    "äººé¡ç¯„ä¾‹": (
        "æ˜¨å¤©åŠ ç­åˆ°åä¸€é»ï¼Œå›å®¶è·¯ä¸Šçªç„¶ä¸‹èµ·äº†å¤§é›¨ï¼Œè·¯é‚Šæ”¤çš„è±†æ¼¿é‚„æ˜¯æº«çš„ï¼Œ"
        "å–å®Œæ‰è¦ºå¾—é€™é€±æœ«ä¸€å®šè¦è£œå€‹çœ ã€‚"
    ),
    "å­¸è¡“ç¯„ä¾‹": (
        "The experiment demonstrates that introducing a lightweight regularization term "
        "improves generalization on small datasets without significantly increasing "
        "computational cost."
    ),
}

if "input_text" not in st.session_state:
    st.session_state.input_text = ""


def load_sample(name: str):
    st.session_state.input_text = sample_texts[name]


col_left, col_right = st.columns([3, 1])
with col_left:
    st.text_area(
        "è¼¸å…¥æ–‡å­—",
        key="input_text",
        height=200,
        placeholder="è²¼ä¸Šè¦åµæ¸¬çš„å…§å®¹ï¼Œæˆ–ä½¿ç”¨å³å´ç¯„ä¾‹å¿«é€Ÿæ¸¬è©¦ã€‚",
    )
with col_right:
    st.markdown("ç¯„ä¾‹æ–‡æœ¬")
    for label in sample_texts.keys():
        st.button(f"è¼‰å…¥ {label}", on_click=load_sample, args=(label,))

uploaded_file = st.file_uploader("æˆ–ä¸Šå‚³ç´”æ–‡å­—æª” (.txt)", type=["txt"])

text_from_file = ""
if uploaded_file is not None:
    text_from_file = read_uploaded_file(uploaded_file)
    if text_from_file:
        st.success("å·²è®€å–æª”æ¡ˆå…§å®¹ï¼Œå°‡å„ªå…ˆä½¿ç”¨æª”æ¡ˆæ–‡å­—é€²è¡Œåˆ¤å®šã€‚")
    else:
        st.warning("æª”æ¡ˆå…§å®¹ç„¡æ³•è§£ç¢¼ï¼Œè«‹ç¢ºèªç‚º UTF-8 ç´”æ–‡å­—ã€‚")

st.markdown("---")

if st.button("é–‹å§‹åµæ¸¬"):
    text = text_from_file or st.session_state.input_text
    if not text.strip():
        st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—æˆ–ä¸Šå‚³æª”æ¡ˆã€‚")
    else:
        with st.spinner("æ¨¡å‹æ¨è«–ä¸­ï¼Œè«‹ç¨å€™..."):
            result = predict(text)

        if result is None:
            st.error("æœªå–å¾—æœ‰æ•ˆè¼¸å…¥ï¼Œè«‹é‡è©¦ã€‚")
        else:
            ai_prob, human_prob, max_conf, breakdown = result
            st.subheader("çµæœ")
            st.write(
                f"AI ç”Ÿæˆæ©Ÿç‡ï¼š**{ai_prob * 100:.1f}%** | äººé¡æ’°å¯«æ©Ÿç‡ï¼š**{human_prob * 100:.1f}%**"
            )

            bar_ai, bar_human = st.columns(2)
            with bar_ai:
                st.progress(ai_prob)
                st.caption("AI ç”Ÿæˆ")
            with bar_human:
                st.progress(human_prob)
                st.caption("äººé¡æ’°å¯«")

            label = "AI ç”Ÿæˆ" if ai_prob >= human_prob else "äººé¡æ’°å¯«"
            st.info(f"æ¨¡å‹åˆ¤æ–·ï¼š**{label}**")

            with st.expander("æ¨¡å‹ç´°ç¯€", expanded=False):
                for k, v in breakdown.items():
                    st.write(f"{k}: {v:.3f}")

            if max_conf < 0.6:
                st.warning("æ¨¡å‹ä¿¡å¿ƒåä½ï¼Œçµæœåƒ…ä¾›åƒè€ƒã€‚")

st.markdown("---")
st.caption(
    "éš±ç§æç¤ºï¼šæ‰€æœ‰æ¨è«–åƒ…åœ¨æœ¬åœ°ç«¯åŸ·è¡Œï¼Œä¸æœƒä¸Šå‚³æˆ–å„²å­˜æ‚¨çš„æ–‡æœ¬ã€‚è¼¸å…¥éçŸ­æ™‚ï¼Œæ¨¡å‹ä¿¡å¿ƒå¯èƒ½è¼ƒä½ã€‚"
)
